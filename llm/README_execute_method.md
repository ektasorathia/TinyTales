# Generate Story Tool - Execute Method

## Overview

The `GenerateStoryTool` now includes a fully functional `execute` method that integrates with LLM services to generate picture stories based on user prompts.

## Features

### âœ… **ToolInterface Implementation**
- Implements the framework's `ToolInterface` abstract base class
- Provides required class variables: `name` and `description`
- Includes schema methods: `get_input_schema()` and `get_output_schema()`

### âœ… **LLM Integration**
- Uses LangChain for LLM interactions
- Supports OpenAI models (default) and Ollama (optional)
- Implements proper prompt engineering with structured prompts
- Includes fallback to mock generation if LLM fails

### âœ… **Execute Method**
```python
async def execute(self, input_data: GenerateStoryRequest, token: Optional[Dict[str, Any]] = None) -> GenerateStoryResponse
```

**Features:**
- **Input Validation**: Validates input data using Pydantic schemas
- **LLM Story Generation**: Calls `_generate_story_with_llm()` method
- **Response Parsing**: Parses JSON response from LLM
- **Error Handling**: Comprehensive error handling with fallbacks
- **Metadata**: Includes detailed metadata in response

## Setup Instructions

### 1. **Install Dependencies**
```bash
cd llm
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. **Configure Environment**
Create a `.env` file in the `llm` directory:
```bash
# For OpenAI
# OPENAI_API_KEY=your_openai_api_key_here
# use_ollama=false
# model_name=gpt-3.5-turbo

# For Ollama (alternative)
use_ollama=true
ollama_model=llama2
ollama_base_url=http://localhost:11434
```

### 3. **Test the Execute Method**

#### **Structure Test (No API Key Required)**
```bash
python test_execute_structure.py
```

#### **Full Test (Requires API Key)**
```bash
python test_execute_method.py
```

## Usage Examples

### **Direct Tool Usage**
```python
from tools.generate_story.tool import GenerateStoryTool
from tools.generate_story.schemas import GenerateStoryRequest

# Create tool instance
tool = GenerateStoryTool()

# Create request
request = GenerateStoryRequest(
    username="TestUser",
    prompt="A brave little mouse who discovers a magical garden",
    age_group="3",
    scene_count=5,
    genre="kids"
)

# Execute the tool
response = await tool.execute(request)

# Check results
if response.success:
    print(f"Story title: {response.data.title}")
    print(f"Number of scenes: {len(response.data.scenes)}")
    for scene in response.data.scenes:
        print(f"Scene {scene.id}: {scene.description}")
        print(f"Image prompt: {scene.imagePrompt}")
else:
    print(f"Error: {response.error}")
```

### **Via FastAPI Endpoint**
```bash
curl -X POST http://localhost:8000/generate-story \
  -H "Content-Type: application/json" \
  -d '{
    "username": "TestUser",
    "prompt": "A brave little mouse"
  }'
```

## LLM Integration Details

### **Prompt Engineering**
The tool uses structured prompts from `prompts/story_generation.py`:
- **STORY_GENERATION_PROMPT**: Main prompt for story generation
- **IMAGE_GENERATION_PROMPT**: Prompt for image descriptions
- **STORY_VALIDATION_PROMPT**: Prompt for story validation

### **Response Format**
The LLM is expected to return JSON in this format:
```json
{
  "title": "Story Title",
  "scenes": [
    {
      "id": 1,
      "description": "Scene description",
      "imagePrompt": "Image generation prompt"
    }
  ]
}
```

### **Fallback Mechanism**
If the LLM fails or returns invalid JSON:
1. Logs the error
2. Falls back to mock story generation
3. Ensures minimum scene count
4. Returns a valid response

## Error Handling

The execute method includes comprehensive error handling:
- **LLM Connection Errors**: Falls back to mock generation
- **JSON Parsing Errors**: Falls back to mock generation
- **Validation Errors**: Returns error response with details
- **General Exceptions**: Returns error response with message

## Configuration Options

### **LLM Settings** (in `.env` or `config.toml`)
- `model_name`: LLM model to use (default: "gpt-3.5-turbo")
- `max_tokens`: Maximum tokens for response (default: 1000)
- `temperature`: Creativity level (default: 0.7)
- `enable_prompt_logging`: Log prompts and responses (default: true)

### **Story Settings**
- `age_group`: Target age group ("3", "children", "young_adult", "adult")
- `scene_count`: Number of scenes (minimum 5)
- `genre`: Story genre ("kids", "fantasy", "adventure", etc.)

## Troubleshooting

### **"No module named 'pychomsky'" Error**
- âœ… **Fixed**: Updated LLM manager to use standard LangChain models
- âœ… **Solution**: Use `langchain-openai` instead of `pychomsky`

### **"OPENAI_API_KEY not set" Error**
- Set your OpenAI API key in the `.env` file
- Or use Ollama by setting `use_ollama=true`

### **"LLM response parsing failed" Error**
- The tool will automatically fall back to mock generation
- Check the logs for the actual LLM response

## Next Steps

1. **Set up your API key** in the `.env` file
2. **Test the execute method** with real LLM calls
3. **Integrate with your application** using the FastAPI endpoints
4. **Customize prompts** in `prompts/story_generation.py`
5. **Add image generation** integration for the image prompts

## Files Modified

- âœ… `tools/generate_story/tool.py` - Added execute method and LLM integration
- âœ… `app/llm/manager.py` - Updated to use standard LangChain models
- âœ… `requirements.txt` - Updated dependencies
- âœ… `test_execute_structure.py` - Structure test script
- âœ… `test_execute_method.py` - Full test script (requires API key)
- âœ… `.env` - Environment configuration template
- âœ… `README_execute_method.md` - This documentation

The execute method is now ready for production use! ðŸŽ‰
